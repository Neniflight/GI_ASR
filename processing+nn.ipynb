{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch \n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you read**, do the preparation steps mentioned in the processing_lr.ipynb notebook to get the proper packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing / Feature Engineering\n",
    "Here, we are feeding our voice lines through wav2vec2 and convering them into waveforms that we can use for training!\n",
    "\n",
    "We went through the same steps for processing and feature engineering as in processing_lr.ipynb, so you can **skip** through this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using mac, pip install sox\n",
    "# otherwise, pip install PySoundFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wav2Vec2 takes a waveform directly and outputs higher quality features or \n",
    "latent representations according to Neural Speech Recognition Lecture. \n",
    "Here, we are using Wav2Vec2 to go from acoustics to tensors using the \n",
    "processor to voicing embedding by transformers. Note that in class, \n",
    "we used Wav2Vec2 to go from waves to words, but here we go from waves \n",
    "to voice embeddings! This high dimensional embedding captures the speaker \n",
    "identity (tone, pitch, and accent), prosody, and phonetic content. \n",
    "\"\"\"\n",
    "\n",
    "def extract_voice_embeddings(audio_file):\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "    # resample the wav file to 16000 bc Wav2Vec2 is trained on those files\n",
    "    # a perfect resample of this voiceline is not possible. According to the \n",
    "    # Nyquist Theorem, the highest freq is captured by a sample signal is one half\n",
    "    # the sampling rate. The highest freq by a human voice is up to 20kHz, so the\n",
    "    # frequencies of the voice should be captured for the most part according to the theorem\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "\n",
    "    waveform = F.normalize(waveform)\n",
    "    # change waveform to mono if it is stereo bc Wav2Vec2 is trained on that\n",
    "    if waveform.ndimension() == 2:\n",
    "        waveform = waveform.squeeze(0)\n",
    "        \n",
    "    # process the wavefrom into inputs \n",
    "    inputs = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = inputs['input_values'].to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_values).last_hidden_state\n",
    "    # create unique voice embeddings for each character \n",
    "    voice_embedding = torch.mean(embeddings, dim=1).squeeze().cpu().numpy()\n",
    "    return voice_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_voice_embeddings(\"characters/Albedo/79_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"characters\"\n",
    "embeddings = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_folder = [i for i in os.listdir(data_dir) if '.wav' not in i]\n",
    "char_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going through each character \n",
    "for character in char_folder:\n",
    "    character_dir = os.path.join(data_dir, character)\n",
    "    print(f\"Currently on Character: {character}\")\n",
    "    if os.path.isdir(character_dir):\n",
    "        for file_name in os.listdir(character_dir):\n",
    "            file_path = os.path.join(character_dir, file_name)\n",
    "            if file_path.endswith(\".wav\"):\n",
    "                # create embedding for each wav file\n",
    "                embedding = extract_voice_embeddings(file_path)\n",
    "                embeddings.append(embedding)\n",
    "                # assign labels aka characters to each one \n",
    "                labels.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(embeddings)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Here, we are using a deep neural network to run on the voice embeddings. SID that is text independent is a nonlinear task that involves a lot of high dimensional data, which means that it can capture intricate variations in speech patterns. However, according to class in Neural Network I and II, we have zero idea how they are making their decisions and requires way more computation than a normal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# fit the scaler to data to standarize the data to have mean 0 and variance of 1\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Do a 80/20 split between training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a neural network for voice classification\n",
    "class VoiceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(VoiceClassifier, self).__init__()\n",
    "        # first fully connected layer with BatchNorm and 512 units \n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "\n",
    "        #second fully connected layer with BatchNorm and 256 units\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        #define third fully connected layer with BatchNorm and 128 yunits\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        #define output layer with the num_classes (82 characters))\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "\n",
    "        #preventing overfitting by randomly setting some neurons to 0\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    # function to pass input thorugh each layer and applying activation functions\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert splits into appropriate tensors \n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor([char_folder.index(label) for label in y_train], dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor([char_folder.index(label) for label in y_test], dtype=torch.long)\n",
    "\n",
    "# create tensordataset objects\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# dataloader objects process batches and shuffle the training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up initial parameters for neural network\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(char_folder)\n",
    "vc_model = VoiceClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vc_model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop going through num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    vc_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # iterate over batches of data from train_loader\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass and compute the loss\n",
    "        outputs = vc_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate running loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    vc_model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = vc_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            # get predicted class\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    \n",
    "    # calculate avg validation loss and accuracy for each epoch\n",
    "    val_loss /= len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # early stop logic: we reset counter if there is improvement and otherwise we keep counting\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    # if we do not improve in 5 epochs, we stop and we found the best model\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to predict what character an audio file sounds like\n",
    "def predict_character(audio_file):\n",
    "    vc_model.eval()\n",
    "\n",
    "    #extract voice embeddings and scale accordingly\n",
    "    embedding = extract_voice_embeddings(audio_file)\n",
    "    embedding_scaled = scaler.transform([embedding])\n",
    "    # convert it to a tensor \n",
    "    embedding_tensor = torch.tensor(embedding_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #forward pass with the inputted embedding tensor\n",
    "        output = vc_model(embedding_tensor)\n",
    "        #get the predicted class by finding the index with max score\n",
    "        _, pred = torch.max(output, 1)\n",
    "    \n",
    "    return char_folder[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_character('characters/Lisa/27_audio.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy is found by looking at the accuracy of the last epoch. In our case, we found the accuracy to be aboue 78%, which is quite impressive, considering that we have over 82 characters to predict from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading your own sound\n",
    "\n",
    "This is a copy and paste from the processing_lr.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename, duration, fs=16000):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    write(filename, fs, recording)\n",
    "    print(f\"Recording saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_audio('output.wav', duration=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_character(\"output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_character(\"calvin.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vc_nnmodel.pkl', 'wb') as f:\n",
    "    pickle.dump(vc_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
