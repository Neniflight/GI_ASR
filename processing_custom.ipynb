{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddu\\anaconda3\\envs\\pytorch_audio\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch \n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import parselmouth\n",
    "from parselmouth.praat import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using mac, pip install sox\n",
    "# otherwise, pip install PySoundFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soundfile']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.list_audio_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_file):\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = mfccs.mean(axis=1)\n",
    "    \n",
    "    snd = parselmouth.Sound(audio_file)\n",
    "    pitch = call(snd, \"To Pitch\", 0.0, 75, 600)\n",
    "    mean_pitch = call(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "\n",
    "    formants = call(snd, \"To Formant (burg)\", 0.0, 5, 5500, 0.025, 50)\n",
    "    formant1 = call(formants, \"Get mean\", 1, 0, 0, \"Hertz\")\n",
    "    formant2 = call(formants, \"Get mean\", 2, 0, 0, \"Hertz\")\n",
    "    \n",
    "    features = np.concatenate([mfccs_mean, [mean_pitch, formant1, formant2]])\n",
    "    \n",
    "    if np.isnan(features).any():\n",
    "        print(f\"NaN values found in features from {audio_file}\")\n",
    "        features = np.nan_to_num(features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-161.35746765,   77.45172882,  -20.14035034,   15.95415974,\n",
       "         -7.5135932 ,  -11.78045082,  -12.19209194,   -3.3014617 ,\n",
       "        -14.07195854,  -11.5178175 ,   -8.25163078,   -7.53186035,\n",
       "         -7.88459492,  122.59520166,  782.72179796, 1880.05436535])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_audio_features(\"characters/Albedo/79_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"characters\"\n",
    "embeddings = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albedo',\n",
       " 'Alhaitham',\n",
       " 'Aloy',\n",
       " 'Amber',\n",
       " 'Arataki Itto',\n",
       " 'Baizhu',\n",
       " 'Barbara',\n",
       " 'Beidou',\n",
       " 'Bennett',\n",
       " 'Candace',\n",
       " 'Charlotte',\n",
       " 'Childe',\n",
       " 'Chongyun',\n",
       " 'Clorinde',\n",
       " 'Collei',\n",
       " 'Cyno',\n",
       " 'Dehya',\n",
       " 'Diluc',\n",
       " 'Diona',\n",
       " 'Dori',\n",
       " 'Ei',\n",
       " 'Eula',\n",
       " 'Faruzan',\n",
       " 'Fischl',\n",
       " 'Freminet',\n",
       " 'Furina',\n",
       " 'Ganyu',\n",
       " 'Gorou',\n",
       " 'Hu Tao',\n",
       " 'Jean',\n",
       " 'Kaede',\n",
       " 'Kaedehara Kazuha',\n",
       " 'Kaeya',\n",
       " 'Kamisato Ayaka',\n",
       " 'Kamisato Ayato',\n",
       " 'Kaveh',\n",
       " 'Kazuha',\n",
       " 'Keqing',\n",
       " 'Kirara',\n",
       " 'Klee',\n",
       " 'Kujou Sara',\n",
       " 'Kuki Shinobu',\n",
       " 'Layla',\n",
       " 'Lisa',\n",
       " 'Lynette',\n",
       " 'Lyney',\n",
       " 'Mika',\n",
       " 'Mona',\n",
       " 'Nahida',\n",
       " 'Navia',\n",
       " 'Neuvillette',\n",
       " 'Nilou',\n",
       " 'Ningguang',\n",
       " 'Noelle',\n",
       " 'Paimon',\n",
       " 'Qiqi',\n",
       " 'Raiden Shogun',\n",
       " 'Razor',\n",
       " 'Rosaria',\n",
       " 'Sangonomiya Kokomi',\n",
       " 'Sayu',\n",
       " 'Shenhe',\n",
       " 'Shikanoin Heizou',\n",
       " 'Sucrose',\n",
       " 'Tartaglia',\n",
       " 'Thoma',\n",
       " 'Tighnari',\n",
       " 'Traveler',\n",
       " 'Venti',\n",
       " 'Wanderer',\n",
       " 'Wriothesley',\n",
       " 'Xiangling',\n",
       " 'Xiao',\n",
       " 'Xingqiu',\n",
       " 'Xinyan',\n",
       " 'Yae Miko',\n",
       " 'Yanfei',\n",
       " 'Yaoyao',\n",
       " 'Yelan',\n",
       " 'Yoimiya',\n",
       " 'Yun Jin',\n",
       " 'Zhongli']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_folder = [i for i in os.listdir(data_dir) if '.wav' not in i]\n",
    "char_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values found in features extracted from characters\\Alhaitham\\18_audio.wav\n",
      "NaN values found in features extracted from characters\\Baizhu\\73_audio.wav\n",
      "NaN values found in features extracted from characters\\Candace\\74_audio.wav\n",
      "NaN values found in features extracted from characters\\Childe\\36_audio.wav\n",
      "NaN values found in features extracted from characters\\Childe\\55_audio.wav\n",
      "NaN values found in features extracted from characters\\Cyno\\36_audio.wav\n",
      "NaN values found in features extracted from characters\\Cyno\\64_audio.wav\n",
      "NaN values found in features extracted from characters\\Dehya\\32_audio.wav\n",
      "NaN values found in features extracted from characters\\Freminet\\24_audio.wav\n",
      "NaN values found in features extracted from characters\\Freminet\\4_audio.wav\n",
      "NaN values found in features extracted from characters\\Kaveh\\40_audio.wav\n",
      "NaN values found in features extracted from characters\\Kirara\\70_audio.wav\n",
      "NaN values found in features extracted from characters\\Kujou Sara\\42_audio.wav\n",
      "NaN values found in features extracted from characters\\Kujou Sara\\8_audio.wav\n",
      "NaN values found in features extracted from characters\\Kuki Shinobu\\28_audio.wav\n",
      "NaN values found in features extracted from characters\\Neuvillette\\13_audio.wav\n",
      "NaN values found in features extracted from characters\\Venti\\37_audio.wav\n",
      "NaN values found in features extracted from characters\\Wanderer\\69_audio.wav\n",
      "NaN values found in features extracted from characters\\Wanderer\\94_audio.wav\n",
      "NaN values found in features extracted from characters\\Xiao\\17_audio.wav\n",
      "NaN values found in features extracted from characters\\Xiao\\41_audio.wav\n",
      "NaN values found in features extracted from characters\\Xiao\\64_audio.wav\n",
      "NaN values found in features extracted from characters\\Xingqiu\\11_audio.wav\n",
      "NaN values found in features extracted from characters\\Yelan\\70_audio.wav\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for character in char_folder:\n",
    "    character_dir = os.path.join(data_dir, character)\n",
    "    if os.path.isdir(character_dir):\n",
    "        for file_name in os.listdir(character_dir):\n",
    "            file_path = os.path.join(character_dir, file_name)\n",
    "            if file_path.endswith(\".wav\"):\n",
    "                features = extract_audio_features(file_path)\n",
    "                embeddings.append(features)\n",
    "                labels.append(character)\n",
    "\n",
    "\n",
    "X = np.array(embeddings)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(VoiceClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(X).any(), \"Input data contains NaNs\"\n",
    "assert not np.isinf(X).any(), \"Input data contains infinite values\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor([char_folder.index(label) for label in y_train], dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor([char_folder.index(label) for label in y_test], dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 3.9093\n",
      "Epoch [2/20], Loss: 3.0883\n",
      "Epoch [3/20], Loss: 2.6918\n",
      "Epoch [4/20], Loss: 2.4693\n",
      "Epoch [5/20], Loss: 2.3295\n",
      "Epoch [6/20], Loss: 2.2380\n",
      "Epoch [7/20], Loss: 2.1506\n",
      "Epoch [8/20], Loss: 2.1373\n",
      "Epoch [9/20], Loss: 2.0732\n",
      "Epoch [10/20], Loss: 2.0492\n",
      "Epoch [11/20], Loss: 2.0101\n",
      "Epoch [12/20], Loss: 1.9612\n",
      "Epoch [13/20], Loss: 1.9441\n",
      "Epoch [14/20], Loss: 1.9438\n",
      "Epoch [15/20], Loss: 1.9481\n",
      "Epoch [16/20], Loss: 1.8950\n",
      "Epoch [17/20], Loss: 1.8711\n",
      "Epoch [18/20], Loss: 1.8808\n",
      "Epoch [19/20], Loss: 1.8389\n",
      "Epoch [20/20], Loss: 1.8190\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(char_folder)\n",
    "classifier_model = VoiceClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    classifier_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(char_folder)\n",
    "vc_model = VoiceClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vc_model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.9085, Val Loss: 3.1658, Accuracy: 0.3479\n",
      "Epoch [2/100], Loss: 3.0638, Val Loss: 2.4658, Accuracy: 0.4616\n",
      "Epoch [3/100], Loss: 2.6566, Val Loss: 2.0787, Accuracy: 0.5397\n",
      "Epoch [4/100], Loss: 2.4511, Val Loss: 1.8701, Accuracy: 0.5721\n",
      "Epoch [5/100], Loss: 2.3227, Val Loss: 1.7337, Accuracy: 0.5984\n",
      "Epoch [6/100], Loss: 2.2103, Val Loss: 1.6926, Accuracy: 0.5909\n",
      "Epoch [7/100], Loss: 2.1477, Val Loss: 1.5813, Accuracy: 0.6352\n",
      "Epoch [8/100], Loss: 2.1114, Val Loss: 1.5435, Accuracy: 0.6096\n",
      "Epoch [9/100], Loss: 2.0759, Val Loss: 1.5453, Accuracy: 0.6209\n",
      "Epoch [10/100], Loss: 2.0329, Val Loss: 1.4855, Accuracy: 0.6340\n",
      "Epoch [11/100], Loss: 1.9974, Val Loss: 1.4754, Accuracy: 0.6390\n",
      "Epoch [12/100], Loss: 2.0033, Val Loss: 1.4547, Accuracy: 0.6402\n",
      "Epoch [13/100], Loss: 1.9559, Val Loss: 1.4491, Accuracy: 0.6390\n",
      "Epoch [14/100], Loss: 1.9008, Val Loss: 1.3833, Accuracy: 0.6540\n",
      "Epoch [15/100], Loss: 1.9206, Val Loss: 1.3813, Accuracy: 0.6596\n",
      "Epoch [16/100], Loss: 1.9224, Val Loss: 1.3771, Accuracy: 0.6577\n",
      "Epoch [17/100], Loss: 1.8801, Val Loss: 1.3487, Accuracy: 0.6602\n",
      "Epoch [18/100], Loss: 1.8559, Val Loss: 1.3916, Accuracy: 0.6458\n",
      "Epoch [19/100], Loss: 1.8367, Val Loss: 1.3547, Accuracy: 0.6596\n",
      "Epoch [20/100], Loss: 1.8607, Val Loss: 1.3605, Accuracy: 0.6571\n",
      "Epoch [21/100], Loss: 1.8348, Val Loss: 1.3721, Accuracy: 0.6633\n",
      "Epoch [22/100], Loss: 1.8130, Val Loss: 1.3125, Accuracy: 0.6777\n",
      "Epoch [23/100], Loss: 1.7925, Val Loss: 1.3138, Accuracy: 0.6721\n",
      "Epoch [24/100], Loss: 1.8087, Val Loss: 1.3299, Accuracy: 0.6633\n",
      "Epoch [25/100], Loss: 1.7958, Val Loss: 1.3082, Accuracy: 0.6740\n",
      "Epoch [26/100], Loss: 1.7891, Val Loss: 1.3038, Accuracy: 0.6677\n",
      "Epoch [27/100], Loss: 1.7812, Val Loss: 1.3048, Accuracy: 0.6633\n",
      "Epoch [28/100], Loss: 1.7579, Val Loss: 1.3038, Accuracy: 0.6721\n",
      "Epoch [29/100], Loss: 1.7382, Val Loss: 1.3052, Accuracy: 0.6783\n",
      "Epoch [30/100], Loss: 1.7665, Val Loss: 1.2816, Accuracy: 0.6758\n",
      "Epoch [31/100], Loss: 1.7588, Val Loss: 1.3075, Accuracy: 0.6740\n",
      "Epoch [32/100], Loss: 1.7318, Val Loss: 1.2837, Accuracy: 0.6771\n",
      "Epoch [33/100], Loss: 1.7651, Val Loss: 1.2602, Accuracy: 0.6771\n",
      "Epoch [34/100], Loss: 1.7223, Val Loss: 1.2926, Accuracy: 0.6827\n",
      "Epoch [35/100], Loss: 1.7079, Val Loss: 1.2825, Accuracy: 0.6715\n",
      "Epoch [36/100], Loss: 1.7134, Val Loss: 1.2796, Accuracy: 0.6646\n",
      "Epoch [37/100], Loss: 1.7098, Val Loss: 1.2788, Accuracy: 0.6665\n",
      "Epoch [38/100], Loss: 1.6921, Val Loss: 1.2763, Accuracy: 0.6702\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    vc_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = vc_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    vc_model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = vc_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_character(audio_file):\n",
    "    vc_model.eval()\n",
    "    features = extract_audio_features(audio_file)\n",
    "    features_scaled = scaler.transform([features])\n",
    "    features_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = vc_model(features_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "    \n",
    "    return char_folder[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lisa'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_character('characters/Lisa/27_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename, duration, fs=16000):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    write(filename, fs, recording)\n",
    "    print(f\"Recording saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording saved to output.wav\n"
     ]
    }
   ],
   "source": [
    "record_audio('output.wav', duration=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Razor'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_character(\"calvin.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
