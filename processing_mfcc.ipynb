{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch \n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using mac, pip install sox\n",
    "# otherwise, pip install PySoundFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soundfile']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.list_audio_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddu\\anaconda3\\envs\\pytorch_audio\\Lib\\site-packages\\transformers\\configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(audio_file, n_mfcc=13):\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    \n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "    \n",
    "    mfcc_transform = T.MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc)\n",
    "    mfcc = mfcc_transform(waveform)\n",
    "    \n",
    "    mfcc = mfcc.mean(dim=2).squeeze().numpy()\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddu\\anaconda3\\envs\\pytorch_audio\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-178.0971   ,   42.131977 ,  -31.356077 ,   -3.3366582,\n",
       "        -18.424139 ,  -22.077742 ,  -19.098751 ,   -6.620508 ,\n",
       "        -11.038792 ,  -12.467265 ,   -5.432606 ,   -6.685038 ,\n",
       "         -7.3597755], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_mfcc_features(\"characters/Albedo/79_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"characters\"\n",
    "embeddings = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albedo',\n",
       " 'Alhaitham',\n",
       " 'Aloy',\n",
       " 'Amber',\n",
       " 'Arataki Itto',\n",
       " 'Baizhu',\n",
       " 'Barbara',\n",
       " 'Beidou',\n",
       " 'Bennett',\n",
       " 'Candace',\n",
       " 'Charlotte',\n",
       " 'Childe',\n",
       " 'Chongyun',\n",
       " 'Clorinde',\n",
       " 'Collei',\n",
       " 'Cyno',\n",
       " 'Dehya',\n",
       " 'Diluc',\n",
       " 'Diona',\n",
       " 'Dori',\n",
       " 'Ei',\n",
       " 'Eula',\n",
       " 'Faruzan',\n",
       " 'Fischl',\n",
       " 'Freminet',\n",
       " 'Furina',\n",
       " 'Ganyu',\n",
       " 'Gorou',\n",
       " 'Hu Tao',\n",
       " 'Jean',\n",
       " 'Kaede',\n",
       " 'Kaedehara Kazuha',\n",
       " 'Kaeya',\n",
       " 'Kamisato Ayaka',\n",
       " 'Kamisato Ayato',\n",
       " 'Kaveh',\n",
       " 'Kazuha',\n",
       " 'Keqing',\n",
       " 'Kirara',\n",
       " 'Klee',\n",
       " 'Kujou Sara',\n",
       " 'Kuki Shinobu',\n",
       " 'Layla',\n",
       " 'Lisa',\n",
       " 'Lynette',\n",
       " 'Lyney',\n",
       " 'Mika',\n",
       " 'Mona',\n",
       " 'Nahida',\n",
       " 'Navia',\n",
       " 'Neuvillette',\n",
       " 'Nilou',\n",
       " 'Ningguang',\n",
       " 'Noelle',\n",
       " 'Paimon',\n",
       " 'Qiqi',\n",
       " 'Raiden Shogun',\n",
       " 'Razor',\n",
       " 'Rosaria',\n",
       " 'Sangonomiya Kokomi',\n",
       " 'Sayu',\n",
       " 'Shenhe',\n",
       " 'Shikanoin Heizou',\n",
       " 'Sucrose',\n",
       " 'Tartaglia',\n",
       " 'Thoma',\n",
       " 'Tighnari',\n",
       " 'Traveler',\n",
       " 'Venti',\n",
       " 'Wanderer',\n",
       " 'Wriothesley',\n",
       " 'Xiangling',\n",
       " 'Xiao',\n",
       " 'Xingqiu',\n",
       " 'Xinyan',\n",
       " 'Yae Miko',\n",
       " 'Yanfei',\n",
       " 'Yaoyao',\n",
       " 'Yelan',\n",
       " 'Yoimiya',\n",
       " 'Yun Jin',\n",
       " 'Zhongli']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_folder = [i for i in os.listdir(data_dir) if '.wav' not in i]\n",
    "char_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(data_dir):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for character in char_folder:\n",
    "        character_dir = os.path.join(data_dir, character)\n",
    "        if os.path.isdir(character_dir):\n",
    "            for file_name in os.listdir(character_dir):\n",
    "                file_path = os.path.join(character_dir, file_name)\n",
    "                if file_path.endswith(\".wav\"):\n",
    "                    mfcc = extract_mfcc_features(file_path)\n",
    "                    features.append(mfcc)\n",
    "                    labels.append(character)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "X, y = extract_features_and_labels(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(VoiceClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor([char_folder.index(label) for label in y_train], dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor([char_folder.index(label) for label in y_test], dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(char_folder)\n",
    "vc_model = VoiceClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vc_model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.9106, Val Loss: 3.1464, Accuracy: 0.3498\n",
      "Epoch [2/100], Loss: 3.0857, Val Loss: 2.4992, Accuracy: 0.4653\n",
      "Epoch [3/100], Loss: 2.6756, Val Loss: 2.1221, Accuracy: 0.5209\n",
      "Epoch [4/100], Loss: 2.4713, Val Loss: 1.9095, Accuracy: 0.5484\n",
      "Epoch [5/100], Loss: 2.3316, Val Loss: 1.7989, Accuracy: 0.5597\n",
      "Epoch [6/100], Loss: 2.2743, Val Loss: 1.6816, Accuracy: 0.5715\n",
      "Epoch [7/100], Loss: 2.1751, Val Loss: 1.6692, Accuracy: 0.5771\n",
      "Epoch [8/100], Loss: 2.1654, Val Loss: 1.6044, Accuracy: 0.5878\n",
      "Epoch [9/100], Loss: 2.1474, Val Loss: 1.5848, Accuracy: 0.5978\n",
      "Epoch [10/100], Loss: 2.0800, Val Loss: 1.5828, Accuracy: 0.5871\n",
      "Epoch [11/100], Loss: 2.0782, Val Loss: 1.5551, Accuracy: 0.5890\n",
      "Epoch [12/100], Loss: 2.0700, Val Loss: 1.4998, Accuracy: 0.6121\n",
      "Epoch [13/100], Loss: 2.0388, Val Loss: 1.5030, Accuracy: 0.6115\n",
      "Epoch [14/100], Loss: 1.9968, Val Loss: 1.4729, Accuracy: 0.6234\n",
      "Epoch [15/100], Loss: 1.9796, Val Loss: 1.4751, Accuracy: 0.6096\n",
      "Epoch [16/100], Loss: 1.9912, Val Loss: 1.4776, Accuracy: 0.6052\n",
      "Epoch [17/100], Loss: 1.9658, Val Loss: 1.4516, Accuracy: 0.6215\n",
      "Epoch [18/100], Loss: 1.9270, Val Loss: 1.4308, Accuracy: 0.6296\n",
      "Epoch [19/100], Loss: 1.9109, Val Loss: 1.4349, Accuracy: 0.6115\n",
      "Epoch [20/100], Loss: 1.9067, Val Loss: 1.4321, Accuracy: 0.6284\n",
      "Epoch [21/100], Loss: 1.9207, Val Loss: 1.4320, Accuracy: 0.6152\n",
      "Epoch [22/100], Loss: 1.9285, Val Loss: 1.4114, Accuracy: 0.6159\n",
      "Epoch [23/100], Loss: 1.8701, Val Loss: 1.4218, Accuracy: 0.6034\n",
      "Epoch [24/100], Loss: 1.8946, Val Loss: 1.4088, Accuracy: 0.6290\n",
      "Epoch [25/100], Loss: 1.8623, Val Loss: 1.3983, Accuracy: 0.6190\n",
      "Epoch [26/100], Loss: 1.8589, Val Loss: 1.4050, Accuracy: 0.6177\n",
      "Epoch [27/100], Loss: 1.8615, Val Loss: 1.3860, Accuracy: 0.6177\n",
      "Epoch [28/100], Loss: 1.9041, Val Loss: 1.3909, Accuracy: 0.6184\n",
      "Epoch [29/100], Loss: 1.8462, Val Loss: 1.4217, Accuracy: 0.6077\n",
      "Epoch [30/100], Loss: 1.8345, Val Loss: 1.4047, Accuracy: 0.6084\n",
      "Epoch [31/100], Loss: 1.8141, Val Loss: 1.3689, Accuracy: 0.6284\n",
      "Epoch [32/100], Loss: 1.7968, Val Loss: 1.3669, Accuracy: 0.6284\n",
      "Epoch [33/100], Loss: 1.8225, Val Loss: 1.3958, Accuracy: 0.6171\n",
      "Epoch [34/100], Loss: 1.8078, Val Loss: 1.3774, Accuracy: 0.6259\n",
      "Epoch [35/100], Loss: 1.7963, Val Loss: 1.3406, Accuracy: 0.6334\n",
      "Epoch [36/100], Loss: 1.7839, Val Loss: 1.3679, Accuracy: 0.6259\n",
      "Epoch [37/100], Loss: 1.7994, Val Loss: 1.3766, Accuracy: 0.6196\n",
      "Epoch [38/100], Loss: 1.8036, Val Loss: 1.3456, Accuracy: 0.6384\n",
      "Epoch [39/100], Loss: 1.8151, Val Loss: 1.3642, Accuracy: 0.6240\n",
      "Epoch [40/100], Loss: 1.7871, Val Loss: 1.3475, Accuracy: 0.6240\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    vc_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = vc_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    vc_model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = vc_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_character(audio_file):\n",
    "    vc_model.eval()\n",
    "    mfcc = extract_mfcc_features(audio_file)\n",
    "    mfcc_scaled = scaler.transform([mfcc])\n",
    "    mfcc_tensor = torch.tensor(mfcc_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = vc_model(mfcc_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "    \n",
    "    return char_folder[pred.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddu\\anaconda3\\envs\\pytorch_audio\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lisa'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_character('characters/Lisa/27_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename, duration, fs=16000):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    write(filename, fs, recording)\n",
    "    print(f\"Recording saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording saved to output.wav\n"
     ]
    }
   ],
   "source": [
    "record_audio('output.wav', duration=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddu\\anaconda3\\envs\\pytorch_audio\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Dehya'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_character(\"calvin.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddu\\anaconda3\\envs\\pytorch_audio\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ei'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_character(\"output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
