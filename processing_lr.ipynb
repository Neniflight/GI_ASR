{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch \n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before this step, ensure that you create your **environment** using the **environment.yml** file that was provided in the Github. That file contains all the needed packages to ensure this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if you do not have the characters folders from our preprocessing step, look at our preprocessing notebook or download them from the following link: [Characters.zip](https://drive.google.com/file/d/1q3AdK38yMUIf4CRcbGazdCUVDl8n2RNB/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pull off a speaker identification task (SID) that is text independent, our model should receive a speech sample X and then through a neural network or model, determine the speaker of the sample. This notebook consists of processing the wav files for each character, training the model for **Logistic Regression**, and allow you to upload your own voice to see which character you sound like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing / Feature Engineering\n",
    "Here, we are feeding our voice lines through wav2vec2 and convering them into waveforms that we can use for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using mac, pip install sox\n",
    "# otherwise, pip install PySoundFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.list_audio_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to speed up the computation for preprocessing by using your GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processors and model from Wav2Vec2\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wav2Vec2 takes a waveform directly and outputs higher quality features or \n",
    "latent representations according to Neural Speech Recognition Lecture. \n",
    "Here, we are using Wav2Vec2 to go from acoustics to tensors using the \n",
    "processor to voicing embedding by transformers. Note that in class, \n",
    "we used Wav2Vec2 to go from waves to words, but here we go from waves \n",
    "to voice embeddings! This high dimensional embedding captures the speaker \n",
    "identity (tone, pitch, and accent), prosody, and phonetic content. \n",
    "\"\"\"\n",
    "\n",
    "def extract_voice_embeddings(audio_file):\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "    # resample the wav file to 16000 bc Wav2Vec2 is trained on those files\n",
    "    # a perfect resample of this voiceline is not possible. According to the \n",
    "    # Nyquist Theorem, the highest freq is captured by a sample signal is one half\n",
    "    # the sampling rate. The highest freq by a human voice is up to 20kHz, so the\n",
    "    # frequencies of the voice should be captured for the most part according to the theorem\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "\n",
    "    waveform = F.normalize(waveform)\n",
    "    # change waveform to mono if it is stereo bc Wav2Vec2 is trained on that\n",
    "    if waveform.ndimension() == 2:\n",
    "        waveform = waveform.squeeze(0)\n",
    "        \n",
    "    # process the wavefrom into inputs \n",
    "    inputs = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = inputs['input_values'].to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_values).last_hidden_state\n",
    "    # create unique voice embeddings for each character \n",
    "    voice_embedding = torch.mean(embeddings, dim=1).squeeze().cpu().numpy()\n",
    "    return voice_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_voice_embeddings(\"data/characters/Albedo/0_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/characters\"\n",
    "embeddings = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the character folder\n",
    "char_folder = [i for i in os.listdir(data_dir) if '.wav' not in i]\n",
    "char_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in char_folder:\n",
    "    character_dir = os.path.join(data_dir, character)\n",
    "    print(f\"Currently on Character: {character}\")\n",
    "    if os.path.isdir(character_dir):\n",
    "        for file_name in os.listdir(character_dir):\n",
    "            file_path = os.path.join(character_dir, file_name)\n",
    "            if file_path.endswith(\".wav\"):\n",
    "                # create embedding for each wav file\n",
    "                embedding = extract_voice_embeddings(file_path)\n",
    "                embeddings.append(embedding)\n",
    "                # assign labels aka characters to each one \n",
    "                labels.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(embeddings)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the X and y variables as csv files, so you don't need to \n",
    "# go through processing again\n",
    "np.savetxt(\"X.csv\", X, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"y.csv\", y, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "If you want to skip processing the data, start by extracting the processed data from the respective csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't want to set up CUDA and process text again,\n",
    "# use this https://drive.google.com/file/d/1aMqL2mr9FmrDFtpVe6CoIwlpG33ZJ-XN/view?usp=sharing\n",
    "X = np.genfromtxt(\"X.csv\", delimiter=\",\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(pd.read_csv(\"y.csv\", header=None).loc[:,0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each character label into a numeric label \n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a testing and training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression on 500 iterations\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an audio file, it will use the model to predict a character you\n",
    "# sound like\n",
    "def predict_character(audio_file):\n",
    "    # feed the embedding through processing\n",
    "    embedding = extract_voice_embeddings(audio_file)\n",
    "    embedding = embedding.reshape(1, -1)\n",
    "    # predict the numeric label for a character\n",
    "    pred = clf.predict(embedding)\n",
    "    # convert from numeric label to character name \n",
    "    character = label_encoder.inverse_transform(pred)\n",
    "    return character[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_character(\"data/characters/Yun Jin/0_audio.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading your own sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename, duration, fs=16000):\n",
    "    print(\"Recording...\")\n",
    "    #records the sound in 16000 Hz sample rate and mono channel\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    write(filename, fs, recording)\n",
    "    print(f\"Recording saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start speaking into your microphone!\n",
    "record_audio('output.wav', duration=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feminine voice like in feminine_voice.wav was identified as Razor,\n",
    "# which is a very animal-like male character. This seems quite off, so \n",
    "# this model probably is not the best for this task. \n",
    "predict_character(\"feminine_voice.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
