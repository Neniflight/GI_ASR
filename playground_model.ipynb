{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use this notebook to play around and see which character you sound like or different audio files! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "\n",
    "data_dir = \"characters\"\n",
    "char_folder = [i for i in os.listdir(data_dir) if '.wav' not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a neural network for voice classification\n",
    "class VoiceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(VoiceClassifier, self).__init__()\n",
    "        # first fully connected layer with BatchNorm and 512 units \n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "\n",
    "        #second fully connected layer with BatchNorm and 256 units\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        #define third fully connected layer with BatchNorm and 128 yunits\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        #define output layer with the num_classes (82 characters))\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "\n",
    "        #preventing overfitting by randomly setting some neurons to 0\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    # function to pass input thorugh each layer and applying activation functions\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vc_nnmodel.pkl', 'rb') as f:\n",
    "    vc_model = pickle.load(f)\n",
    "\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_model.to(device)\n",
    "vc_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename, duration, fs=16000):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    write(filename, fs, recording)\n",
    "    print(f\"Recording saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_character(audio_file):\n",
    "    vc_model.eval()\n",
    "    \n",
    "    def extract_voice_embeddings(audio_file):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "            sample_rate = 16000\n",
    "\n",
    "        waveform = F.normalize(waveform)\n",
    "        if waveform.ndimension() == 2:\n",
    "            waveform = waveform.squeeze(0)\n",
    "            \n",
    "        inputs = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        input_values = inputs['input_values'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(input_values).last_hidden_state\n",
    "        # create unique voice embeddings for each character \n",
    "        voice_embedding = torch.mean(embeddings, dim=1).squeeze().cpu().numpy()\n",
    "        return voice_embedding\n",
    "    \n",
    "    # Extract voice embeddings from the audio file\n",
    "    embedding = extract_voice_embeddings(audio_file)\n",
    "    \n",
    "    # Scale the extracted embedding using the fitted scaler\n",
    "    embedding_scaled = scaler.transform([embedding])\n",
    "    \n",
    "    # Convert the scaled embedding to a PyTorch tensor\n",
    "    embedding_tensor = torch.tensor(embedding_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Perform inference (no gradient computation needed)\n",
    "    with torch.no_grad():\n",
    "        output = vc_model(embedding_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "    \n",
    "    # Return the predicted character label\n",
    "    return char_folder[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_audio('output.wav', duration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_character(\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
